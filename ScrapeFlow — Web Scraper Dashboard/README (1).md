# ğŸ•·ï¸ ScrapeFlow â€” Web Scraper & Data Pipeline Dashboard

A real-time web scraper dashboard with live progress tracking, log streaming, data table output, and CSV/JSON export. Built to demonstrate automation pipeline UX.

## ğŸš€ Features

- **Configurable Scraper** â€” Set URL, data type, CSS selectors, max pages, headless mode, proxy rotation, deduplication
- **Job Queue Manager** â€” View and switch between multiple concurrent scraping jobs
- **Live Progress Bar** â€” Real-time progress %, pages scraped, items extracted, error count, speed
- **Streaming Log Console** â€” Terminal-style log with color-coded levels (info, success, warn, error, data)
- **Scraped Data Table** â€” Paginated, scrollable table with priority tags
- **Export Options** â€” CSV download, JSON download, webhook push

## ğŸ›  Tech Stack

- Single-file HTML/CSS/JS â€” zero dependencies
- Canvas-free progress visualization
- Simulated async scraping pipeline with setInterval
- Real CSV/JSON blob download via `URL.createObjectURL`

## ğŸ“‚ File Structure

```
02-web-scraper-dashboard/
â”œâ”€â”€ index.html    # Complete application
â””â”€â”€ README.md     # This file
```

## ğŸƒ Quick Start

```bash
open index.html
# or
npx serve .
```

## ğŸ’¡ How to Demo

1. Enter any URL in the form (e.g. `https://amazon.com`)
2. Select data type: Products, Leads, News, Jobs, or Reviews
3. Click **â–¶ Start Scraping**
4. Watch the live log, progress bar, and data table populate
5. Click **CSV** or **JSON** to download the extracted data

## ğŸ”Œ Production Integration

```python
# Real scraper backend with Playwright
from playwright.async_api import async_playwright
import asyncio, json

async def scrape(url, selector):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url)
        items = await page.query_selector_all(selector)
        data = [await el.inner_text() for el in items]
        await browser.close()
        return data

# Send progress to frontend via WebSocket
# ws.send(json.dumps({"progress": 45, "items": 312}))
```

## ğŸ—ï¸ Architecture

```
Frontend (this) â†â†’ WebSocket â†â†’ Scraper Backend
                                    â†“
                              Proxy Pool
                                    â†“
                              Data Pipeline
                                    â†“
                          Database / Webhook
```

## ğŸ“ License

MIT
